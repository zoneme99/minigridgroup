{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c13378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "True\n",
      "None\n",
      "Observation Space: (84, 84, 3)\n",
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Starting Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "HIP error: invalid device function\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m model = PPO(\u001b[33m\"\u001b[39m\u001b[33mCnnPolicy\u001b[39m\u001b[33m\"\u001b[39m, vec_env, verbose=\u001b[32m1\u001b[39m, batch_size=\u001b[32m512\u001b[39m, learning_rate=\u001b[32m1e-4\u001b[39m, device=device)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Finished!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 5. Save the Champion\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rocm-pytorch/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rocm-pytorch/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rocm-pytorch/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:202\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[32m    201\u001b[39m     obs_tensor = obs_as_tensor(\u001b[38;5;28mself\u001b[39m._last_obs, \u001b[38;5;28mself\u001b[39m.device)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     actions, values, log_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m actions = actions.cpu().numpy()\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rocm-pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rocm-pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rocm-pytorch/lib/python3.12/site-packages/stable_baselines3/common/policies.py:645\u001b[39m, in \u001b[36mActorCriticPolicy.forward\u001b[39m\u001b[34m(self, obs, deterministic)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[33;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[32m    639\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    642\u001b[39m \u001b[33;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_features_extractor:\n\u001b[32m    647\u001b[39m     latent_pi, latent_vf = \u001b[38;5;28mself\u001b[39m.mlp_extractor(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rocm-pytorch/lib/python3.12/site-packages/stable_baselines3/common/policies.py:672\u001b[39m, in \u001b[36mActorCriticPolicy.extract_features\u001b[39m\u001b[34m(self, obs, features_extractor)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[33;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[32m    665\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    669\u001b[39m \u001b[33;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_features_extractor:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rocm-pytorch/lib/python3.12/site-packages/stable_baselines3/common/policies.py:130\u001b[39m, in \u001b[36mBaseModel.extract_features\u001b[39m\u001b[34m(self, obs, features_extractor)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs, features_extractor: BaseFeaturesExtractor) -> th.Tensor:\n\u001b[32m    123\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m    Preprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[33;03m    :return: The extracted features\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     preprocessed_obs = \u001b[43mpreprocess_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rocm-pytorch/lib/python3.12/site-packages/stable_baselines3/common/preprocessing.py:120\u001b[39m, in \u001b[36mpreprocess_obs\u001b[39m\u001b[34m(obs, observation_space, normalize_images)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces.Box):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m normalize_images \u001b[38;5;129;01mand\u001b[39;00m is_image_space(observation_space):\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m / \u001b[32m255.0\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obs.float()\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces.Discrete):\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# One hot encoding and convert to float to avoid errors\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: HIP error: invalid device function\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from ctf_env import CaptureTheFlagPZ\n",
    "import os\n",
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\"\n",
    "os.environ[\"AMD_SERIALIZE_KERNEL\"] = \"3\"\n",
    "os.environ[\"TORCH_USE_HIP_DSA\"] = \"1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Initialize PettingZoo Env\n",
    "env = CaptureTheFlagPZ(render_mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.color_reduction_v0(env, mode='full')\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "# Convert to SB3 Vector Env\n",
    "# This allows PPO to see \"Red\" and \"Blue\" as just two independent samples in a batch\n",
    "vec_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "# Concatenate them so PPO trains on 2 agents at once\n",
    "# num_vec_envs=4 to better utilize GPU\n",
    "vec_env = ss.concat_vec_envs_v1(vec_env, num_vec_envs=4, num_cpus=0, base_class='stable_baselines3')\n",
    "\n",
    "print(f\"Observation Space: {vec_env.observation_space.shape}\")\n",
    "# Should be (84, 84, 3) -> 84x84 pixels, 3 stacked frames\n",
    "\n",
    "# Train with PPO\n",
    "# We use CnnPolicy because we are using images\n",
    "# Added device=\"cpu\" to bypass the GPU error\n",
    "# Increased batch size to fully utilize GPU memory\n",
    "model = PPO(\"CnnPolicy\", vec_env, verbose=1, batch_size=512, learning_rate=1e-4, device=device)\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "model.learn(total_timesteps=300000)\n",
    "print(\"Training Finished!\")\n",
    "\n",
    "# 5. Save the Champion\n",
    "model.save(\"ctf_champion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb36330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+rocm6.2\n",
      "CUDA available: True\n",
      "CUDA version: None\n",
      "GPU name: AMD Radeon RX 7700 XT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43d63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To train Gen 2 vs Gen 1, we need to convert the environment so 'Blue' is just a part of the game (like a moving wall).\n"
     ]
    }
   ],
   "source": [
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from ctf_env import CaptureTheFlagPZ\n",
    "\n",
    "#  Load the \"Old\" Champion (Gen 1)\n",
    "# We load it on CPU to avoid errors\n",
    "old_champion = PPO.load(\"ctf_champion\", device=\"cpu\")\n",
    "\n",
    "# 2. Define the Training Environment\n",
    "env = CaptureTheFlagPZ(render_mode=\"rgb_array\")\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.color_reduction_v0(env, mode='full')\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "# 3. The \"Gauntlet\" Wrapper\n",
    "# We need a way to tell the environment: \"Blue actions come from the Old Model, Red actions come from the New Model\"\n",
    "# PettingZoo doesn't support this out of the box easily, so we usually just run a custom loop or\n",
    "# use a library like 'shimmy' to convert it to a Single-Agent Gym environment where the Opponent is part of the environment.\n",
    "\n",
    "print(\"To train Gen 2 vs Gen 1, we need to convert the environment so 'Blue' is just a part of the game (like a moving wall).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91dd141-afcc-4bda-a3f8-88a1059bbe50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ROCM",
   "language": "python",
   "name": "ctf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
