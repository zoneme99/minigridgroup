{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da740c63",
   "metadata": {},
   "source": [
    "The code below should let us vizualise and see the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ff8b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tobbe\\RL_Project\\minigridgroup\\venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from ctf_env import CaptureTheFlagPZ\n",
    "\n",
    "def record_match(red_model, blue_model, filename=\"match_replay.gif\"):\n",
    "    # 1. Setup Env with SAME wrappers as training\n",
    "    env = CaptureTheFlagPZ(render_mode=\"rgb_array\")\n",
    "    \n",
    "    # MUST match training wrappers\n",
    "    # env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    # env = ss.color_reduction_v0(env, mode='full')\n",
    "    # env = ss.frame_stack_v1(env, 3)\n",
    "    \n",
    "    observations, infos = env.reset()\n",
    "    frames = []\n",
    "    \n",
    "    print(f\"Recording match to {filename}...\")\n",
    "    \n",
    "    # PettingZoo Loop\n",
    "    while env.agents:\n",
    "        # Save the full render (not the tiny agent view)\n",
    "        # We access the internal env to get the full high-res map\n",
    "        # Use env.unwrapped.render() to use our custom multi-agent visualizer\n",
    "        frames.append(env.unwrapped.render())\n",
    "        \n",
    "        actions = {}\n",
    "        # (4x)\n",
    "        # 2. Iterate through every agent currently in the game\n",
    "        for agent_id in env.agents:\n",
    "            obs = observations[agent_id]\n",
    "            \n",
    "            formatted_obs = {\n",
    "                \"image\": np.array(obs[\"image\"])[None, :],\n",
    "                \"role\": np.array(obs[\"role\"])[None, :]\n",
    "            }\n",
    "            \n",
    "            if \"red\" in agent_id:\n",
    "                act, _ = red_model.predict(formatted_obs, deterministic=True)\n",
    "            else:\n",
    "                act, _ = blue_model.predict(formatted_obs, deterministic=True)\n",
    "            \n",
    "            # predict returnerar en array [action], vi behöver värdet (int)\n",
    "            actions[agent_id] = int(act[0])\n",
    "            \n",
    "        observations, rewards, terms, truncs, infos = env.step(actions)\n",
    "    \n",
    "    # Save GIF\n",
    "    imageio.mimsave(filename, frames, fps=10)\n",
    "    print(f\"Replay saved! ({len(frames)} frames)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d12354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording match to self_play_match.gif...\n",
      "Replay saved! (800 frames)\n"
     ]
    },
    {
     "data": {
      "image/gif": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- RUN THE MATCH ---\n",
    "# Load the model we just trained\n",
    "# CRITICAL FIX: Added device=\"cpu\"\n",
    "champion_model = PPO.load(\"ctf_final_boss\", device=\"cpu\")\n",
    "\n",
    "# Let the champion play against itself!\n",
    "record_match(champion_model, champion_model, filename=\"self_play_match.gif\")\n",
    "\n",
    "from IPython.display import Image\n",
    "display(Image(filename=\"self_play_match.gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f6763a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Use the champion model for all agents\u001b[39;00m\n\u001b[32m     29\u001b[39m     formatted_obs = {\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: np.array(obs[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[38;5;28;01mNone\u001b[39;00m, :],\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: np.array(obs[\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[32m     32\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     act, _ = \u001b[43mchampion_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     actions[agent_id] = \u001b[38;5;28mint\u001b[39m(act[\u001b[32m0\u001b[39m])\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 2. Step the Environment\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tobbe\\RL_Project\\minigridgroup\\venv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:557\u001b[39m, in \u001b[36mBaseAlgorithm.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m    538\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    539\u001b[39m     observation: Union[np.ndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np.ndarray]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    542\u001b[39m     deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    543\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np.ndarray, ...]]]:\n\u001b[32m    544\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[33;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[33;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    555\u001b[39m \u001b[33;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tobbe\\RL_Project\\minigridgroup\\venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:368\u001b[39m, in \u001b[36mBasePolicy.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    365\u001b[39m obs_tensor, vectorized_env = \u001b[38;5;28mself\u001b[39m.obs_to_tensor(observation)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     actions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[32m    370\u001b[39m actions = actions.cpu().numpy().reshape((-\u001b[32m1\u001b[39m, *\u001b[38;5;28mself\u001b[39m.action_space.shape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tobbe\\RL_Project\\minigridgroup\\venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:717\u001b[39m, in \u001b[36mActorCriticPolicy._predict\u001b[39m\u001b[34m(self, observation, deterministic)\u001b[39m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> th.Tensor:\n\u001b[32m    710\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    711\u001b[39m \u001b[33;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[32m    712\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    715\u001b[39m \u001b[33;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[32m    716\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m.get_actions(deterministic=deterministic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tobbe\\RL_Project\\minigridgroup\\venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:752\u001b[39m, in \u001b[36mActorCriticPolicy.get_distribution\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m    750\u001b[39m features = \u001b[38;5;28msuper\u001b[39m().extract_features(obs, \u001b[38;5;28mself\u001b[39m.pi_features_extractor)\n\u001b[32m    751\u001b[39m latent_pi = \u001b[38;5;28mself\u001b[39m.mlp_extractor.forward_actor(features)\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tobbe\\RL_Project\\minigridgroup\\venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:697\u001b[39m, in \u001b[36mActorCriticPolicy._get_action_dist_from_latent\u001b[39m\u001b[34m(self, latent_pi)\u001b[39m\n\u001b[32m    694\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.action_dist.proba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m.log_std)\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_dist, CategoricalDistribution):\n\u001b[32m    696\u001b[39m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_dist, MultiCategoricalDistribution):\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# Here mean_actions are the flattened logits\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.action_dist.proba_distribution(action_logits=mean_actions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tobbe\\RL_Project\\minigridgroup\\venv\\Lib\\site-packages\\stable_baselines3\\common\\distributions.py:288\u001b[39m, in \u001b[36mCategoricalDistribution.proba_distribution\u001b[39m\u001b[34m(self, action_logits)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mproba_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m: SelfCategoricalDistribution, action_logits: th.Tensor) -> SelfCategoricalDistribution:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28mself\u001b[39m.distribution = \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tobbe\\RL_Project\\minigridgroup\\venv\\Lib\\site-packages\\torch\\distributions\\categorical.py:75\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28mself\u001b[39m.logits = logits - \u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28mself\u001b[39m._param = \u001b[38;5;28mself\u001b[39m.probs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.logits\n\u001b[32m     77\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import supersuit as ss\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "from ctf_env import CaptureTheFlagPZ\n",
    "\n",
    "# --- 1. SETUP ENVIRONMENT ---\n",
    "env = CaptureTheFlagPZ(render_mode=\"rgb_array\")\n",
    "# env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "# env = ss.color_reduction_v0(env, mode='full')\n",
    "# env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "# --- 2. LIVE TOURNAMENT ---\n",
    "wins = {\"red\": 0, \"blue\": 0, \"draw\": 0}\n",
    "total_matches = 10\n",
    "\n",
    "for i in range(total_matches):\n",
    "    observations, _ = env.reset()\n",
    "    step_count = 0\n",
    "    \n",
    "    # In PettingZoo ParallelEnv, the game ends when env.agents is empty\n",
    "    while env.agents:\n",
    "        # 1. Get AI Actions for ALL current agents\n",
    "        actions = {}\n",
    "        for agent_id in env.agents:\n",
    "            obs = observations[agent_id]\n",
    "            # Use the champion model for all agents\n",
    "            formatted_obs = {\n",
    "                \"image\": np.array(obs[\"image\"])[None, :],\n",
    "                \"role\": np.array(obs[\"role\"])[None, :]\n",
    "            }\n",
    "        \n",
    "            act, _ = champion_model.predict(formatted_obs, deterministic=False)\n",
    "            actions[agent_id] = int(act[0])\n",
    "\n",
    "        # 2. Step the Environment\n",
    "        observations, rewards, terms, truncs, infos = env.step(actions)\n",
    "        step_count += 1\n",
    "\n",
    "        # 3. RENDER LIVE\n",
    "        frame = env.unwrapped.render()\n",
    "        \n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Status Strings - Check if ANY agent on a team has the flag\n",
    "        r_carrying = any(env.unwrapped.carrying_flag.get(f\"red_{idx}\", False) for idx in range(2))\n",
    "        b_carrying = any(env.unwrapped.carrying_flag.get(f\"blue_{idx}\", False) for idx in range(2))\n",
    "        \n",
    "        r_status = \"HAS FLAG!\" if r_carrying else \"\"\n",
    "        b_status = \"HAS FLAG!\" if b_carrying else \"\"\n",
    "        \n",
    "        title_str = (f\"MATCH {i+1} / {total_matches}\\n\"\n",
    "                     f\"Red Team: {wins['red']} {r_status}  vs  Blue Team: {wins['blue']} {b_status}\\n\"\n",
    "                     f\"Step: {step_count}\")\n",
    "        plt.title(title_str, fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # --- SPEED CONTROL ---\n",
    "        time.sleep(0.01) \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # 4. Check Game Over and Results\n",
    "        # If any agent terminated, we check the rewards to see who won\n",
    "        if any(terms.values()) or any(truncs.values()):\n",
    "            # Check team-wide rewards (if one red agent gets a high reward, red wins)\n",
    "            red_reward = max([rewards.get(a, 0) for a in rewards if \"red\" in a] + [-1])\n",
    "            blue_reward = max([rewards.get(a, 0) for a in rewards if \"blue\" in a] + [-1])\n",
    "\n",
    "            if red_reward > 5.0:\n",
    "                wins[\"red\"] += 1\n",
    "                print(f\"Match {i+1} Result: RED TEAM CAPTURED THE FLAG!\")\n",
    "                time.sleep(2)\n",
    "            elif blue_reward > 5.0:\n",
    "                wins[\"blue\"] += 1\n",
    "                print(f\"Match {i+1} Result: BLUE TEAM CAPTURED THE FLAG!\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                wins[\"draw\"] += 1\n",
    "                print(f\"Match {i+1} Result: DRAW (Timeout)\")\n",
    "                time.sleep(1)\n",
    "            break # Exit the while loop for this match\n",
    "\n",
    "\n",
    "# --- FINAL RESULTS ---\n",
    "print(\"=\"*30)\n",
    "print(\"TOURNAMENT FINISHED\")\n",
    "print(f\"Final Score: Red {wins['red']} - {wins['blue']} Blue\")\n",
    "print(\"=\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
