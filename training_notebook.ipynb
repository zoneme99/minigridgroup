{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c13378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "False\n",
      "None\n",
      "Observation Space: (84, 84, 3)\n",
      "Loading existing model from ctf_champion.zip...\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\pettingzoo\\utils\\conversions.py:320: UserWarning: The `infos` dictionary returned by `env.reset` was empty. OverwritingAgent IDs will be used as keys\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 285   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 114   |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "Training Finished!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from ctf_env import CaptureTheFlagPZ\n",
    "\n",
    "# --- Load Model ---\n",
    "# Set load_model to True if you want to continue training an existing model\n",
    "load_model = True \n",
    "model_path = \"ctf_champion.zip\"\n",
    "\n",
    "\n",
    "# --- Pick Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "\n",
    "# --- Grid Hyper Parameters ---\n",
    "grid_size = 21\n",
    "center_walls = 2 # The higher the number the fewer the walls  \n",
    "mirrored_walls = 15 # The lower the number the fewer walls\n",
    "# Example \n",
    "# First Training  (Easy) grid_size 12x12, center_walls 6, mirrored_walls 2 \n",
    "# Second Training (Medium) grid_size 16x16, center_walls 4, mirrored_walls 8 \n",
    "# Third Training  (Difficult/Default) grid_size 21x21, center_walls 2, mirrored_walls 15\n",
    "\n",
    "\n",
    "# --- Setup ---\n",
    "env = CaptureTheFlagPZ( \n",
    "    render_mode=\"rgb_array\", \n",
    "    grid_size=grid_size, \n",
    "    center_walls=center_walls, \n",
    "    mirrored_walls=mirrored_walls\n",
    "\n",
    ")\n",
    "\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.color_reduction_v0(env, mode='full')\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "vec_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "vec_env = ss.concat_vec_envs_v1(vec_env, num_vec_envs=4, num_cpus=0, base_class='stable_baselines3')\n",
    "\n",
    "print(f\"Observation Space: {vec_env.observation_space.shape}\")\n",
    "# Should be (84, 84, 3) -> 84x84 pixels, 3 stacked frames\n",
    "\n",
    "\n",
    "# --- Load or Create Model ---\n",
    "if load_model == True and os.path.exists(model_path):\n",
    "    print(f\"Loading existing model from {model_path}...\")\n",
    "    model = PPO.load(model_path, env=vec_env, device=device)\n",
    "\n",
    "else:\n",
    "    print(f\"Training a new model...\")\n",
    "    model = PPO(\n",
    "        \"CnnPolicy\", \n",
    "        vec_env, \n",
    "        verbose=1, \n",
    "        batch_size=512, \n",
    "        learning_rate=1e-4, \n",
    "        ent_coef=0.01,\n",
    "        n_steps=2048,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Train Model ---\n",
    "print(\"Starting Training...\")\n",
    "model.learn(total_timesteps=30)\n",
    "print(\"Training Finished!\")\n",
    "\n",
    "model.save(\"ctf_champion\")\n",
    "# 3 million timesteps takes 444 min (8h) to train on a CPU  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb36330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU name: NVIDIA GeForce RTX 5070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43d63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To train Gen 2 vs Gen 1, we need to convert the environment so 'Blue' is just a part of the game (like a moving wall).\n"
     ]
    }
   ],
   "source": [
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from ctf_env import CaptureTheFlagPZ\n",
    "\n",
    "#  Load the \"Old\" Champion (Gen 1)\n",
    "# We load it on CPU to avoid errors\n",
    "old_champion = PPO.load(\"ctf_champion\", device=\"cpu\")\n",
    "\n",
    "# 2. Define the Training Environment\n",
    "env = CaptureTheFlagPZ(render_mode=\"rgb_array\")\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.color_reduction_v0(env, mode='full')\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "# 3. The \"Gauntlet\" Wrapper\n",
    "# We need a way to tell the environment: \"Blue actions come from the Old Model, Red actions come from the New Model\"\n",
    "# PettingZoo doesn't support this out of the box easily, so we usually just run a custom loop or\n",
    "# use a library like 'shimmy' to convert it to a Single-Agent Gym environment where the Opponent is part of the environment.\n",
    "\n",
    "print(\"To train Gen 2 vs Gen 1, we need to convert the environment so 'Blue' is just a part of the game (like a moving wall).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8032c2d3",
   "metadata": {},
   "source": [
    "# Advanced Training\n",
    "Train the same model multiple times with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303e4841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "False\n",
      "None\n",
      "Training a new model...\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "\n",
      "\n",
      "Starting Training Session 1...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\lambda_wrappers\\observation_lambda.py:68\u001b[39m, in \u001b[36maec_observation_lambda._modify_observation\u001b[39m\u001b[34m(self, agent, observation)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchange_observation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_obs_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: basic_obs_wrapper.<locals>.change_obs() takes 2 positional arguments but 3 were given",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# --- Train Model ---\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting Training Session \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining Session \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Finished!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     92\u001b[39m model.save(model_path+new_suffix)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    216\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:97\u001b[39m, in \u001b[36mVecTransposeImage.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     observations, rewards, dones, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\vector\\sb3_vector_wrapper.py:26\u001b[39m, in \u001b[36mSB3VecEnvWrapper.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     observations, rewards, terminations, truncations, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Note: SB3 expects dones to be an np.array\u001b[39;00m\n\u001b[32m     28\u001b[39m     dones = np.array(\n\u001b[32m     29\u001b[39m         [terminations[i] \u001b[38;5;129;01mor\u001b[39;00m truncations[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(terminations))]\n\u001b[32m     30\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\vector\\concat_vec_env.py:76\u001b[39m, in \u001b[36mConcatVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_saved_actions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\vector\\concat_vec_env.py:84\u001b[39m, in \u001b[36mConcatVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m     81\u001b[39m actions = \u001b[38;5;28mlist\u001b[39m(iterate(\u001b[38;5;28mself\u001b[39m.action_space, actions))\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m venv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vec_envs:\n\u001b[32m     83\u001b[39m     data.append(\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m         \u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconcatenate_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m                \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_envs\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     )\n\u001b[32m     90\u001b[39m     idx += venv.num_envs\n\u001b[32m     91\u001b[39m observations, rewards, terminations, truncations, infos = transpose(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\vector\\markov_vector_wrapper.py:70\u001b[39m, in \u001b[36mMarkovVectorEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m     64\u001b[39m agent_set = \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.par_env.agents)\n\u001b[32m     65\u001b[39m act_dict = {\n\u001b[32m     66\u001b[39m     agent: actions[i]\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.par_env.possible_agents)\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agent_set\n\u001b[32m     69\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m observations, rewards, terms, truncs, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpar_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# adds last observation to info where user can get it\u001b[39;00m\n\u001b[32m     73\u001b[39m terminations = np.fromiter(terms.values(), dtype=\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\generic_wrappers\\utils\\shared_wrapper_util.py:130\u001b[39m, in \u001b[36mshared_wrapper_parr.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[32m    126\u001b[39m     actions = {\n\u001b[32m    127\u001b[39m         agent: \u001b[38;5;28mself\u001b[39m.modifiers[agent].modify_action(action)\n\u001b[32m    128\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m agent, action \u001b[38;5;129;01min\u001b[39;00m actions.items()\n\u001b[32m    129\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     observations, rewards, terminations, truncations, infos = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_modifiers(\u001b[38;5;28mself\u001b[39m.agents)\n\u001b[32m    132\u001b[39m     observations = {\n\u001b[32m    133\u001b[39m         agent: \u001b[38;5;28mself\u001b[39m.modifiers[agent].modify_obs(obs)\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m agent, obs \u001b[38;5;129;01min\u001b[39;00m observations.items()\n\u001b[32m    135\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\pettingzoo\\utils\\wrappers\\base_parallel.py:34\u001b[39m, in \u001b[36mBaseParallelWrapper.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mself\u001b[39m, actions: \u001b[38;5;28mdict\u001b[39m[AgentID, ActionType]\n\u001b[32m     27\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     \u001b[38;5;28mdict\u001b[39m[AgentID, \u001b[38;5;28mdict\u001b[39m],\n\u001b[32m     33\u001b[39m ]:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\pettingzoo\\utils\\conversions.py:206\u001b[39m, in \u001b[36maec_to_parallel_wrapper.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    204\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m got agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.aec_env.agent_selection\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Parallel environment wrapper expects agents to step in a cycle.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m obs, rew, termination, truncation, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maec_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.aec_env.step(actions[agent])\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aec_env.agents:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\pettingzoo\\utils\\env.py:186\u001b[39m, in \u001b[36mAECEnv.last\u001b[39m\u001b[34m(self, observe)\u001b[39m\n\u001b[32m    184\u001b[39m agent = \u001b[38;5;28mself\u001b[39m.agent_selection\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m observation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m observe \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    188\u001b[39m     observation,\n\u001b[32m    189\u001b[39m     \u001b[38;5;28mself\u001b[39m._cumulative_rewards[agent],\n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m     \u001b[38;5;28mself\u001b[39m.infos[agent],\n\u001b[32m    193\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\utils\\base_aec_wrapper.py:35\u001b[39m, in \u001b[36mBaseWrapper.observe\u001b[39m\u001b[34m(self, agent)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     obs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# problem is in this line, the obs is sometimes a different size from the obs space\u001b[39;00m\n\u001b[32m     38\u001b[39m     observation = \u001b[38;5;28mself\u001b[39m._modify_observation(agent, obs)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m observation\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\pettingzoo\\utils\\wrappers\\order_enforcing.py:75\u001b[39m, in \u001b[36mOrderEnforcingWrapper.observe\u001b[39m\u001b[34m(self, agent)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m     74\u001b[39m     EnvLogger.error_observe_before_reset()\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:41\u001b[39m, in \u001b[36mBaseWrapper.observe\u001b[39m\u001b[34m(self, agent)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: AgentID) -> ObsType | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\utils\\base_aec_wrapper.py:38\u001b[39m, in \u001b[36mBaseWrapper.observe\u001b[39m\u001b[34m(self, agent)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[32m     35\u001b[39m     obs = \u001b[38;5;28msuper\u001b[39m().observe(\n\u001b[32m     36\u001b[39m         agent\n\u001b[32m     37\u001b[39m     )  \u001b[38;5;66;03m# problem is in this line, the obs is sometimes a different size from the obs space\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     observation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modify_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m observation\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\lambda_wrappers\\observation_lambda.py:70\u001b[39m, in \u001b[36maec_observation_lambda._modify_observation\u001b[39m\u001b[34m(self, agent, observation)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.change_observation_fn(observation, old_obs_space, agent)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchange_observation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_obs_space\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\generic_wrappers\\basic_wrappers.py:19\u001b[39m, in \u001b[36mbasic_obs_wrapper.<locals>.change_obs\u001b[39m\u001b[34m(obs, obs_space)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchange_obs\u001b[39m(obs, obs_space):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchange_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Repo-Home\\07-Djup-Maskininlärning-MiniGrid-Agile-Bois\\.venv\\Lib\\site-packages\\supersuit\\utils\\basic_transforms\\resize.py:28\u001b[39m, in \u001b[36mchange_observation\u001b[39m\u001b[34m(obs, obs_space, resize)\u001b[39m\n\u001b[32m     26\u001b[39m     obs = obs.reshape(obs.shape + (\u001b[32m1\u001b[39m,))\n\u001b[32m     27\u001b[39m interp_method = \u001b[33m\"\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m linear_interp \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m obs = \u001b[43mtinyscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mysize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterp_method\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obs_space.shape) == \u001b[32m2\u001b[39m:\n\u001b[32m     32\u001b[39m     obs = obs.reshape(obs.shape[:\u001b[32m2\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from ctf_env import CaptureTheFlagPZ\n",
    "\n",
    "# --- Load Model ---\n",
    "# Set load_model to True if you want to continue training an existing model\n",
    "base_folder = \"models\"\n",
    "model_name = \"bob\"\n",
    "# (Add) Continue Training\n",
    "\n",
    "# --- Pick Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "\n",
    "# --- PPO Hyper Parameters ---\n",
    "# Training sets controls how many times you train the model\n",
    "training_sets=3\n",
    "learning_rate=[1e-4, 5e-5, 1e-5]\n",
    "ent_coef=[0.01, 0.01, 0.02]\n",
    "total_timesteps = [1_000_000, 1_000_000, 1_000_000]\n",
    "#total_timesteps = [30, 30, 30]\n",
    "\n",
    "# --- Grid Hyper Parameters ---\n",
    "grid_size = [12,16,21]\n",
    "center_walls = [6,4,2] # The higher the number the fewer the walls  \n",
    "mirrored_walls = [2,6,15] # The lower the number the fewer walls\n",
    "# Example \n",
    "# First Training  (Easy) grid_size 12x12, center_walls 6, mirrored_walls 2 \n",
    "# Second Training (Medium) grid_size 16x16, center_walls 4, mirrored_walls 8 \n",
    "# Third Training  (Difficult/Default) grid_size 21x21, center_walls 2, mirrored_walls 15\n",
    "\n",
    "\n",
    "for i in range(training_sets):\n",
    "    model_path = f\"{base_folder}/{model_name}/{model_name}\" \n",
    "    old_suffix = f\"_v{i}.zip\"\n",
    "    new_suffix = f\"_v{i+1}.zip\"\n",
    "\n",
    "    env = CaptureTheFlagPZ( \n",
    "        render_mode=\"rgb_array\", \n",
    "        grid_size=grid_size[i], \n",
    "        center_walls=center_walls[i], \n",
    "        mirrored_walls=mirrored_walls[i]\n",
    "    )\n",
    "\n",
    "    # --- Setup ---\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.color_reduction_v0(env, mode='full')\n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "    vec_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    vec_env = ss.concat_vec_envs_v1(vec_env, num_vec_envs=4, num_cpus=0, base_class='stable_baselines3')\n",
    "\n",
    "\n",
    "    # --- Create Model ---\n",
    "    if i == 0:\n",
    "\n",
    "        print(f\"Training a new model...\")\n",
    "        model = PPO(\n",
    "            \"CnnPolicy\", \n",
    "            vec_env, \n",
    "            verbose=1, \n",
    "            batch_size=512, \n",
    "            learning_rate=learning_rate[i], \n",
    "            ent_coef=ent_coef[i],\n",
    "            n_steps=2048,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(f\"Loading existing model from {model_path}...\")\n",
    "        model = PPO.load(\n",
    "            model_path+old_suffix, \n",
    "            env=vec_env, \n",
    "            device=device,\n",
    "            custom_objects={\n",
    "                \"learning_rate\": learning_rate[i],\n",
    "                \"ent_coef\": ent_coef[i]\n",
    "            }\n",
    "        )\n",
    " \n",
    "\n",
    "\n",
    "    # --- Train Model ---\n",
    "    print(f\"\\n\\nStarting Training Session {i+1}...\")\n",
    "    model.learn(total_timesteps=total_timesteps[i])\n",
    "    print(f\"Training Session {i+1} Finished!\")\n",
    "\n",
    "    model.save(model_path+new_suffix)\n",
    "    # 3 million timesteps takes 444 min (8h) to train on a CPU  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
